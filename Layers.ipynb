{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115725270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a0ead-bc14-4d64-99a4-d89ae7c7a1ef",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Neural network layers are the fundamental building blocks that transform input data into meaningful representations. Each layer type is designed for specific data structures and tasks. The **shape of the input and output tensors** plays a crucial role in understanding how these layers operate.\n",
    "\n",
    "In what follows, we investigate the most common layer types in PyTorch:\n",
    "\n",
    "1. **nn.Linear**: Fully connected layers for general-purpose transformations\n",
    "2. **nn.Conv1d**: 1D convolutions for sequential/temporal data\n",
    "3. **nn.Conv2d**: 2D convolutions for image data\n",
    "4. **nn.Conv3d**: 3D convolutions for volumetric/video data\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19674a-3b68-45ec-923f-911e7192c272",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.Linear** | $(*, H_{\\text{in}})$ | $(*, H_{\\text{out}})$ | MLPs, classification heads, dense connections, Transformer projections | `in_features`, `out_features`, `bias` |\n",
    "| **nn.Conv1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Time series, audio, text (1D sequences) | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` |\n",
    "| **nn.Conv2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Images, feature maps, 2D spatial data | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "| **nn.Conv3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Videos, medical imaging (CT/MRI), 3D point clouds | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.ConvTranspose1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Audio generation, sequence upsampling, WaveNet decoders | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |\n",
    "| **nn.ConvTranspose2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Image generation (GANs), semantic segmentation decoders, autoencoders | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |\n",
    "| **nn.ConvTranspose3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Video generation, 3D medical image reconstruction, volumetric upsampling | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `output_padding` |\n",
    "\n",
    "\n",
    "Detailed explanations of each layer, including mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913c152-b2d7-401b-b573-3dc034a3956d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184a791-9132-4eb1-9ea3-1b2dbb235a57",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "\n",
    "### Where is it used?\n",
    "\n",
    "The `nn.Linear` layer (also known as a fully connected or dense layer) applies a linear transformation to the incoming data. It is the most fundamental building block in neural networks and is used in:\n",
    "\n",
    "- **Multi-Layer Perceptrons (MLPs)**: The backbone of simple feedforward networks\n",
    "- **Classification heads**: Final layers that map features to class logits\n",
    "- **Transformer architectures**: Q, K, V projections and feed-forward networks\n",
    "- **Autoencoders**: Encoding and decoding dense representations\n",
    "- **Regression tasks**: Mapping features to continuous outputs\n",
    "\n",
    "### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(*, H_{\\text{in}})$ where $*$ means any number of dimensions and $H_{\\text{in}}$ is the number of input features\n",
    "- **Output**: $(*, H_{\\text{out}})$ where all dimensions except the last remain unchanged\n",
    "\n",
    "**Important**: The linear transformation is applied to the **last dimension** only.\n",
    "\n",
    "### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "- `in_features` (int): Size of each input sample (required)\n",
    "- `out_features` (int): Size of each output sample (required)\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- \n",
    "### Mathematical Formulation\n",
    "\n",
    "The linear layer applies the following transformation:\n",
    "\n",
    "$$\n",
    "{y_{\\rm out}}^{i,j,\\cdots, k} = \\omega_{k z} \\, {x_{\\rm in}}^{i,j,\\cdots, z} + b^{k}\n",
    "$$\n",
    "\n",
    "\n",
    "where the indices \"$i,j,\\cdots$\" represent an arbitrary dimension, i.e., $*$. This makes the layer **batch-agnostic**. As we see,\n",
    "- $x$ is the input tensor of shape $(*, H_{\\text{in}})$\n",
    "- $\\omega$ is the weight matrix of shape $(H_{\\text{out}}, H_{\\text{in}})$\n",
    "- $b$ is the bias vector of shape $(H_{\\text{out}})$\n",
    "- $y$ is the output tensor of shape $(*, H_{\\text{out}})$\n",
    "- **Notice**, that, all elements in the arbitrary dimension \"$i,j,\\cdots$\" get exactly the same biases and weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Weights are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{H_{\\text{in}}}$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62e6aaa-46a3-4f58-b540-ccab2157f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([30, 20])\n",
      "Bias shape: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Linear usage\n",
    "# Create a linear layer that maps 20 input features to 30 output features\n",
    "linear = nn.Linear(20, 30)\n",
    "\n",
    "# Check the weight and bias shapes\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d299c4-0284-4d7a-b5e5-01cb687c227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 20])\n",
      "Output shape: torch.Size([128, 30])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple 2D input (batch_size, in_features)\n",
    "batch_size = 128\n",
    "in_features = 20\n",
    "out_features = 30\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "models_weights = linear.weight\n",
    "models_biases = linear.bias\n",
    "x = torch.randn(batch_size, in_features)\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Lets now try to reproduce the output manually:\n",
    "\n",
    "output_manual = torch.einsum('bi,oi->bo', x, models_weights) + models_biases.unsqueeze(0).expand(batch_size, -1)\n",
    "print(f'Outputs match: {torch.all(output_manual==output).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b58cd32-e699-4cd3-be86-d44e5e910935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has bias: False\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Without bias\n",
    "linear_no_bias = nn.Linear(20, 30, bias=False)\n",
    "print(f\"Has bias: {linear_no_bias.bias is not None}\")\n",
    "\n",
    "# Verify the transformation manually\n",
    "x = torch.randn(5, 20)\n",
    "output_layer = linear_no_bias(x)\n",
    "output_manual = x @ linear_no_bias.weight.T  # y = x @ W^T\n",
    "\n",
    "print(f\"Outputs match: {torch.allclose(output_layer, output_manual)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13656ff2-0304-4bba-92b6-d911349b9487",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Convolutional layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb845e-958c-4d18-ac16-ffdd8708b9b6",
   "metadata": {},
   "source": [
    "Before we start discussing the types of convolutional layers, we briefly discuss some of the important padding types appearing in these layers:\n",
    "\n",
    "\n",
    "\n",
    "| Padding | Conv1d | Conv2d | Conv3d | Meaning |\n",
    "|---------|--------|--------|--------|---------|\n",
    "| `padding=0` | `0` | `(0,0)` | `(0,0,0)` | No padding, output shrinks |\n",
    "| `padding='valid'` | `0` | `(0,0)` | `(0,0,0)` | Equivalent to `padding=0` |\n",
    "| `padding='same'` | auto | auto | auto | Output size = Input size (requires `stride=1`) |\n",
    "| `padding=k//2` | `k//2` | `(k//2, k//2)` | `(k//2, k//2, k//2)` | Preserves size for odd kernel $k$ with `stride=1`, `dilation=1` |\n",
    "\n",
    "**Common recipe to preserve spatial dimensions:**\n",
    "\n",
    "For `kernel_size=k`, `stride=1`, `dilation=1`:\n",
    "```python\n",
    "# These are equivalent:\n",
    "padding = 'same'\n",
    "padding = k // 2   # only works for odd k\n",
    "```\n",
    "\n",
    "**Examples preserving size:**\n",
    "\n",
    "| Kernel | Padding |\n",
    "|--------|---------|\n",
    "| 3 | 1 |\n",
    "| 5 | 2 |\n",
    "| 7 | 3 |\n",
    "\n",
    "**General formula:** `padding = (kernel_size - 1) // 2` for odd kernels with `stride=1`, `dilation=1`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd3a3e-f067-4665-9c67-47d3cea77666",
   "metadata": {},
   "source": [
    "### nn.Conv1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead6b5e4-7460-4208-a79a-f9afa8a79915",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv1d` layer applies a 1D convolution over an input signal composed of several input planes. It is primarily used for:\n",
    "\n",
    "- **Time series analysis**: Stock prices, sensor data, weather patterns\n",
    "- **Audio processing**: Speech recognition, music classification\n",
    "- **Text/NLP**: Character-level or word-level sequence modeling\n",
    "- **Signal processing**: Any 1D sequential data\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, L_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $L_{\\text{in}}$ = length of the input sequence\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, L_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $L_{\\text{out}} = \\left\\lfloor\\frac{L_{\\text{in}} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, \n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int): Stride of the convolution. Default: `1`\n",
    "- `padding` (int or str): Zero-padding added to both sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size})$\n",
    "\n",
    "\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, L)$ and output of size $(B, C_{\\text{out}}, L_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, i]$ to $y_{\\rm out}[b, c, i] $ is performed as follows:\n",
    "\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s\\cdot i+~d\\cdot k] \\omega[c_{\\rm out}, c_{\\rm in}, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, i] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k=0}^{K -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\cdot \\frac{C_{\\rm in}}{g} + j,~ s\\cdot i+~d\\cdot k] \\omega[c_{\\rm out}, j, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor, it is simply equal to $x_{\\rm in}$ if padding is zero. Moreover, $g = $ `groups`, $s = $ `stride`, $d = $ `dilation` , and $m(c_{\\rm out}) = {\\rm floor}( c_{out} \\cdot  g / C_{out} )$.\n",
    "\n",
    "\n",
    "\n",
    "**Remark**: When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b7daae-f2d9-4635-8b7b-0a0c813d89ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv1d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3\n",
    "conv1d = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv1d.weight.shape}\")  # (out_channels, in_channels, kernel_size)\n",
    "print(f\"Bias shape: {conv1d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef7343c-7a6b-470d-b2f7-0aa8d0f6314e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 48])\n",
      "Expected L_out: 48\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic convolution without padding\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "out_channels = 33\n",
    "kernel_size = 3\n",
    "L_in = 50  # Input sequence length\n",
    "\n",
    "conv1d = nn.Conv1d(in_channels, out_channels, kernel_size)\n",
    "x = torch.randn(batch_size, in_channels, L_in)\n",
    "output = conv1d(x)\n",
    "\n",
    "# Calculate expected output length: L_out = (L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1\n",
    "L_out_expected = (L_in + 2*0 - 1*(kernel_size-1) - 1) // 1 + 1\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected L_out: {L_out_expected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf0b2f29-7ca5-4397-86db-edfce62a1f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n",
      "Length preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Convolution with padding to maintain spatial dimension\n",
    "# For kernel_size=3, padding=1 maintains the length (with stride=1)\n",
    "conv1d_same = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Length preserved: {x.shape[2] == output.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cdef7b-db36-4c24-86ec-062b584ba662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 50])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using padding='same' (requires stride=1)\n",
    "conv1d_same_str = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=5, padding='same')\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_same_str(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b44ee49-aaa2-4797-9aa8-0f15c580aa73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 24])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Convolution with stride > 1 (downsampling)\n",
    "conv1d_stride = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, stride=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_stride(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 1*(3-1) - 1) // 2 + 1 = (50 - 2 - 1) // 2 + 1 = 47//2 + 1 = 23 + 1 = 24\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8463110e-6e6a-4ded-84f3-9160e63f4877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 46])\n",
      "Effective kernel size with dilation=2: 5\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Dilated convolution (for larger receptive field)\n",
    "conv1d_dilated = nn.Conv1d(in_channels=16, out_channels=33, kernel_size=3, dilation=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv1d_dilated(x)\n",
    "\n",
    "# L_out = (50 + 2*0 - 2*(3-1) - 1) / 1 + 1 = (50 - 4 - 1) + 1 = 46\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Effective kernel size with dilation=2: {2*(3-1)+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf59a20-95b9-434e-b493-0bb69e661f1e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.Conv2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1760984-8345-4afd-9d9e-0fb3d61e3eaa",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv2d` layer applies a 2D convolution over an input signal composed of several input planes. It is the backbone of computer vision and is used in:\n",
    "\n",
    "- **Image classification**: CNNs for recognizing objects in images\n",
    "- **Object detection**: YOLO, Faster R-CNN, etc.\n",
    "- **Semantic segmentation**: U-Net, DeepLab, etc.\n",
    "- **Image generation**: GANs, VAEs\n",
    "- **Feature extraction**: Any 2D spatial data processing\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels (e.g., 3 for RGB images)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $H_{\\text{out}} = \\left\\lfloor\\frac{H_{\\text{in}} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$\n",
    "  - $W_{\\text{out}} = \\left\\lfloor\\frac{W_{\\text{in}} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution. Default: `1`\n",
    "- `padding` (int, tuple, or str): Zero-padding added to both sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1])$\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, h, w]$ to $y_{\\rm out}[b, c, h, w]$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, h, w] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s_h h+d_h k_h,~ s_w w + d_w k_w] \\cdot \\omega[c_{\\rm out}, c_{\\rm in}, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{out}, h, w] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\frac{C_{\\rm in}}{g} + j,~ s_h h+d_h \\cdot k_h,~ s_w \\cdot w + d_w \\cdot k_w] \\cdot \\omega[c_{\\rm out}, j, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor (equal to $x_{\\rm in}$ if padding is zero). Moreover, $g = $ `groups`, $(s_h, s_w) = $ `stride`, $(d_h, d_w) = $ `dilation`, $(K_h, K_w) = $ `kernel_size`, and $m(c_{\\rm out}) = {\\rm floor}( c_{\\rm out} \\cdot g / C_{\\rm out} )$.\n",
    "\n",
    "**Remarks**:\n",
    "1. When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes that $(K_h, K_w) = (k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7b128f8-027a-4813-9789-c58536435884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv2d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3\n",
    "conv2d = nn.Conv2d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv2d.weight.shape}\")  # (out_channels, in_channels, kH, kW)\n",
    "print(f\"Bias shape: {conv2d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61d1f869-6b7c-4aae-ad42-87c4790d9f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 24, 49])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Square kernels and equal stride\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "H, W = 50, 100\n",
    "\n",
    "conv2d = nn.Conv2d(16, 33, kernel_size=3, stride=2)\n",
    "x = torch.randn(batch_size, in_channels, H, W)\n",
    "output = conv2d(x)\n",
    "\n",
    "# H_out = (50 + 2*0 - 1*(3-1) - 1) / 2 + 1 = 24\n",
    "# W_out = (100 + 2*0 - 1*(3-1) - 1) / 2 + 1 = 49\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd9059dd-9f38-4efa-b14c-953fbf49f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 28, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Non-square kernels and unequal stride with padding\n",
    "conv2d = nn.Conv2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(4, 2))\n",
    "x = torch.randn(20, 16, 50, 100)\n",
    "output = conv2d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c233100c-c32f-4bc4-9f8f-c33174810412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 224, 224])\n",
      "Output shape: torch.Size([1, 64, 224, 224])\n",
      "Spatial dimensions preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Maintaining spatial dimensions with appropriate padding\n",
    "# For kernel_size=3 and stride=1, padding=1 preserves dimensions\n",
    "conv2d_same = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "x = torch.randn(1, 3, 224, 224)  # Typical ImageNet input\n",
    "output = conv2d_same(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Spatial dimensions preserved: {x.shape[2:] == output.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8804a96c-1f34-4154-9bb9-90b32aa3f7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 26, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Dilated convolution\n",
    "conv2d_dilated = nn.Conv2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\n",
    "x = torch.randn(20, 16, 50, 100)\n",
    "output = conv2d_dilated(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cce597ec-b623-40b1-8df3-bb8dd314b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depthwise conv weight shape: torch.Size([32, 1, 3, 3])\n",
      "Input shape: torch.Size([1, 32, 64, 64])\n",
      "Output shape: torch.Size([1, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Depthwise convolution (groups = in_channels)\n",
    "# Each input channel is convolved separately with its own filter\n",
    "in_channels = 32\n",
    "conv2d_depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels)\n",
    "\n",
    "print(f\"Depthwise conv weight shape: {conv2d_depthwise.weight.shape}\")\n",
    "# Note: each filter only sees 1 input channel (in_channels/groups = 32/32 = 1)\n",
    "\n",
    "x = torch.randn(1, in_channels, 64, 64)\n",
    "output = conv2d_depthwise(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61e131f4-214f-4a72-ab15-a37020a8c7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard conv weight shape: torch.Size([8, 4, 3, 3])\n",
      "Grouped conv weight shape: torch.Size([8, 2, 3, 3])\n",
      "Depthwise conv weight shape: torch.Size([4, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Understanding groups parameter more deeply\n",
    "# Standard conv: each output channel sees ALL input channels\n",
    "# groups=2: split into 2 parallel convs, each seeing half the input channels\n",
    "\n",
    "# Standard convolution\n",
    "conv_standard = nn.Conv2d(4, 8, kernel_size=3, groups=1)\n",
    "print(f\"Standard conv weight shape: {conv_standard.weight.shape}\")\n",
    "# Shape: (8, 4, 3, 3) - each of 8 filters sees all 4 input channels\n",
    "\n",
    "# Grouped convolution with groups=2\n",
    "conv_grouped = nn.Conv2d(4, 8, kernel_size=3, groups=2)\n",
    "print(f\"Grouped conv weight shape: {conv_grouped.weight.shape}\")\n",
    "# Shape: (8, 2, 3, 3) - each of 8 filters sees only 2 input channels (4/2)\n",
    "\n",
    "# Depthwise convolution (groups = in_channels)\n",
    "conv_depthwise = nn.Conv2d(4, 4, kernel_size=3, groups=4)\n",
    "print(f\"Depthwise conv weight shape: {conv_depthwise.weight.shape}\")\n",
    "# Shape: (4, 1, 3, 3) - each of 4 filters sees only 1 input channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "094d0667-849a-4715-92b2-ec099bf8a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
      "  Calculated: (64, 64), Actual: torch.Size([64, 64])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (32, 32), Actual: torch.Size([32, 32])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 5, 'stride': 2, 'padding': 2}\n",
      "  Calculated: (32, 32), Actual: torch.Size([32, 32])\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 2}\n",
      "  Calculated: (62, 62), Actual: torch.Size([62, 62])\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula\n",
    "def conv2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"Calculate Conv2d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    \n",
    "    H_out = (H_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1) // stride[0] + 1\n",
    "    W_out = (W_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1) // stride[1] + 1\n",
    "    return H_out, W_out\n",
    "\n",
    "# Test with various configurations\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 5, 'stride': 2, 'padding': 2},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1, 'dilation': 2},\n",
    "]\n",
    "\n",
    "H_in, W_in = 64, 64\n",
    "x = torch.randn(1, 3, H_in, W_in)\n",
    "\n",
    "for config in configs:\n",
    "    conv = nn.Conv2d(3, 16, **config)\n",
    "    output = conv(x)\n",
    "    H_out_calc, W_out_calc = conv2d_output_size(H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({H_out_calc}, {W_out_calc}), Actual: {output.shape[2:]}\")\n",
    "    print(f\"  Match: {(H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004df0b8-2300-47a6-87dd-8fa3fc4b6e3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conv3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937dd36d-d58e-436f-8d34-c063eaa3316a",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.Conv3d` layer applies a 3D convolution over an input signal composed of several input planes. It is used for:\n",
    "\n",
    "- **Video analysis**: Action recognition, video classification\n",
    "- **Medical imaging**: CT scans, MRI volumes, 3D ultrasound\n",
    "- **3D object recognition**: Point cloud processing, voxel-based analysis\n",
    "- **Spatiotemporal data**: Any data with 3 spatial/temporal dimensions\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $D_{\\text{in}}$ = depth of the input (e.g., number of frames in video, slices in CT scan)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels (number of filters)\n",
    "  - $D_{\\text{out}} = \\left\\lfloor\\frac{D_{\\text{in}} + 2 \\times \\text{padding}[0] - \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) - 1}{\\text{stride}[0]} + 1\\right\\rfloor$\n",
    "  - $H_{\\text{out}} = \\left\\lfloor\\frac{H_{\\text{in}} + 2 \\times \\text{padding}[1] - \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) - 1}{\\text{stride}[1]} + 1\\right\\rfloor$\n",
    "  - $W_{\\text{out}} = \\left\\lfloor\\frac{W_{\\text{in}} + 2 \\times \\text{padding}[2] - \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) - 1}{\\text{stride}[2]} + 1\\right\\rfloor$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "          dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input volume (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution. Default: `1`\n",
    "- `padding` (int, tuple, or str): Zero-padding added to all sides. Default: `0`. Can be `'same'` or `'valid'`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): `'zeros'`, `'reflect'`, `'replicate'`, or `'circular'`. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1], \\text{kernel\\_size}[2])$\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the convolution from $x_{\\rm in}[b, c, d, h, w]$ to $y_{\\rm out}[b, c, d, h, w]$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d, h, w] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in} -1} \\sum_{k_d=0}^{K_d -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ c_{\\rm in},~ s_d \\cdot d + d_d \\cdot k_d,~ s_h \\cdot h + d_h \\cdot k_h,~ s_w \\cdot w + d_w \\cdot k_w] \\cdot \\omega[c_{\\rm out}, c_{\\rm in}, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* For `groups`$ > 1$ and, `in_channels % groups`$~=~$ `out_channels % groups`$=0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d, h, w] = \\sum_{j=0}^{C_{\\rm in}/g -1} \\sum_{k_d=0}^{K_d -1} \\sum_{k_h=0}^{K_h -1} \\sum_{k_w=0}^{K_w -1}~{\\tilde x}_{\\rm in}[b,~ m(c_{\\rm out})\\frac{C_{\\rm in}}{g} + j,~ s_d \\cdot d + d_d\\cdot  k_d,~ s_h \\cdot h + d_h \\cdot k_h,~ s_w\\cdot  w + d_w\\cdot  k_w] \\cdot \\omega[c_{\\rm out}, j, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where ${\\tilde x}_{\\rm in}$ is the padded input tensor (equal to $x_{\\rm in}$ if padding is zero). Moreover, $g = $ `groups`, $(s_d, s_h, s_w) = $ `stride`, $(d_d, d_h, d_w) = $ `dilation`, $(K_d, K_h, K_w) = $ `kernel_size`, and $m(c_{\\rm out}) = \\lfloor c_{\\rm out} \\cdot g / C_{\\rm out} \\rfloor$.\n",
    "\n",
    "**Remarks**:\n",
    "1. When `groups` $=1$, each filter combines information from all input channels. Otherwise, channels are processed in isolated groups, reducing computation and parameters but limiting cross-channel interaction.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes that $(K_d, K_h, K_w) = (k, k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e405e332-e46f-49c3-9935-5d1fe77fd6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([33, 16, 3, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Conv3d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3x3\n",
    "conv3d = nn.Conv3d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv3d.weight.shape}\")  # (out_channels, in_channels, kD, kH, kW)\n",
    "print(f\"Bias shape: {conv3d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3f492e4d-5df9-457b-9946-324e171df263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 4, 24, 49])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Cubic kernels and equal stride\n",
    "conv3d = nn.Conv3d(16, 33, kernel_size=3, stride=2)\n",
    "x = torch.randn(20, 16, 10, 50, 100)  # (batch, channels, depth, height, width)\n",
    "output = conv3d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee98c502-aeff-459f-9972-9b6e52244732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 8, 50, 99])\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Non-cubic kernels and unequal stride with padding\n",
    "# kernel_size=(depth, height, width), stride=(sD, sH, sW), padding=(pD, pH, pW)\n",
    "conv3d = nn.Conv3d(16, 33, kernel_size=(3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))\n",
    "x = torch.randn(20, 16, 10, 50, 100)\n",
    "output = conv3d(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7afc1698-64f6-4025-ba53-7c4045515b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape (video): torch.Size([4, 3, 16, 112, 112])\n",
      "Output shape: torch.Size([4, 64, 16, 112, 112])\n",
      "Dimensions preserved: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Video processing (batch, channels, frames, height, width)\n",
    "# A typical video input: 3 RGB channels, 16 frames, 112x112 resolution\n",
    "batch_size = 4\n",
    "channels = 3\n",
    "frames = 16\n",
    "height = width = 112\n",
    "\n",
    "conv3d_video = nn.Conv3d(channels, 64, kernel_size=3, stride=1, padding=1)\n",
    "video_input = torch.randn(batch_size, channels, frames, height, width)\n",
    "output = conv3d_video(video_input)\n",
    "\n",
    "print(f\"Input shape (video): {video_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Dimensions preserved: {video_input.shape[2:] == output.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58538fa3-8575-432c-8dfb-df7c0e3b0e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CT scan input shape: torch.Size([1, 1, 32, 128, 128])\n",
      "Feature map output shape: torch.Size([1, 16, 32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Medical imaging (CT scan volume)\n",
    "# Input: single channel (grayscale), 32 slices, 128x128 resolution\n",
    "conv3d_medical = nn.Conv3d(1, 16, kernel_size=3, padding=1)\n",
    "ct_scan = torch.randn(1, 1, 32, 128, 128)  # (batch, channel, slices, height, width)\n",
    "output = conv3d_medical(ct_scan)\n",
    "\n",
    "print(f\"CT scan input shape: {ct_scan.shape}\")\n",
    "print(f\"Feature map output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c643439a-2f67-4f6b-926e-f7dbd5c72358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 1}\n",
      "  Calculated: (16, 64, 64), Actual: (16, 64, 64)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (8, 32, 32), Actual: (8, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': (3, 5, 5), 'stride': (1, 2, 2), 'padding': (1, 2, 2)}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for Conv3d\n",
    "def conv3d_output_size(D_in, H_in, W_in, kernel_size, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"Calculate Conv3d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "    \n",
    "    D_out = (D_in + 2*padding[0] - dilation[0]*(kernel_size[0]-1) - 1) // stride[0] + 1\n",
    "    H_out = (H_in + 2*padding[1] - dilation[1]*(kernel_size[1]-1) - 1) // stride[1] + 1\n",
    "    W_out = (W_in + 2*padding[2] - dilation[2]*(kernel_size[2]-1) - 1) // stride[2] + 1\n",
    "    return D_out, H_out, W_out\n",
    "\n",
    "# Test\n",
    "D_in, H_in, W_in = 16, 64, 64\n",
    "x = torch.randn(1, 3, D_in, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': (3, 5, 5), 'stride': (1, 2, 2), 'padding': (1, 2, 2)},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv = nn.Conv3d(3, 16, **config)\n",
    "    output = conv(x)\n",
    "    D_out_calc, H_out_calc, W_out_calc = conv3d_output_size(D_in, H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({D_out_calc}, {H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(D_out_calc, H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55972c90-f365-4dd0-808a-ca8e8555c105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26cbbd43-b45f-4221-afae-ec1a2d374fa7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d2426c73-5afa-455d-885c-4a1c91bdc121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LAYER COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "nn.Linear(512, 256):\n",
      "  Input shape:  (32, 512) -> (batch, in_features)\n",
      "  Output shape: (32, 256) -> (batch, out_features)\n",
      "  Weight shape: (256, 512) -> (out_features, in_features)\n",
      "  Parameters:   131,328\n",
      "\n",
      "nn.Conv1d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (32, 64, 100) -> (batch, in_channels, length)\n",
      "  Output shape: (32, 128, 100) -> (batch, out_channels, length)\n",
      "  Weight shape: (128, 64, 3) -> (out_ch, in_ch, kernel)\n",
      "  Parameters:   24,704\n",
      "\n",
      "nn.Conv2d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (32, 64, 56, 56) -> (batch, in_channels, H, W)\n",
      "  Output shape: (32, 128, 56, 56) -> (batch, out_channels, H, W)\n",
      "  Weight shape: (128, 64, 3, 3) -> (out_ch, in_ch, kH, kW)\n",
      "  Parameters:   73,856\n",
      "\n",
      "nn.Conv3d(64, 128, kernel_size=3, padding=1):\n",
      "  Input shape:  (4, 64, 16, 56, 56) -> (batch, in_channels, D, H, W)\n",
      "  Output shape: (4, 128, 16, 56, 56) -> (batch, out_channels, D, H, W)\n",
      "  Weight shape: (128, 64, 3, 3, 3) -> (out_ch, in_ch, kD, kH, kW)\n",
      "  Parameters:   221,312\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison of all layers\n",
    "print(\"=\" * 80)\n",
    "print(\"LAYER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# nn.Linear\n",
    "linear = nn.Linear(512, 256)\n",
    "x_linear = torch.randn(32, 512)\n",
    "print(f\"\\nnn.Linear(512, 256):\")\n",
    "print(f\"  Input shape:  {tuple(x_linear.shape)} -> (batch, in_features)\")\n",
    "print(f\"  Output shape: {tuple(linear(x_linear).shape)} -> (batch, out_features)\")\n",
    "print(f\"  Weight shape: {tuple(linear.weight.shape)} -> (out_features, in_features)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in linear.parameters()):,}\")\n",
    "\n",
    "# nn.Conv1d\n",
    "conv1d = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv1d = torch.randn(32, 64, 100)\n",
    "print(f\"\\nnn.Conv1d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv1d.shape)} -> (batch, in_channels, length)\")\n",
    "print(f\"  Output shape: {tuple(conv1d(x_conv1d).shape)} -> (batch, out_channels, length)\")\n",
    "print(f\"  Weight shape: {tuple(conv1d.weight.shape)} -> (out_ch, in_ch, kernel)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv1d.parameters()):,}\")\n",
    "\n",
    "# nn.Conv2d\n",
    "conv2d = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv2d = torch.randn(32, 64, 56, 56)\n",
    "print(f\"\\nnn.Conv2d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv2d.shape)} -> (batch, in_channels, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv2d(x_conv2d).shape)} -> (batch, out_channels, H, W)\")\n",
    "print(f\"  Weight shape: {tuple(conv2d.weight.shape)} -> (out_ch, in_ch, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv2d.parameters()):,}\")\n",
    "\n",
    "# nn.Conv3d\n",
    "conv3d = nn.Conv3d(64, 128, kernel_size=3, padding=1)\n",
    "x_conv3d = torch.randn(4, 64, 16, 56, 56)\n",
    "print(f\"\\nnn.Conv3d(64, 128, kernel_size=3, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_conv3d.shape)} -> (batch, in_channels, D, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv3d(x_conv3d).shape)} -> (batch, out_channels, D, H, W)\")\n",
    "print(f\"  Weight shape: {tuple(conv3d.weight.shape)} -> (out_ch, in_ch, kD, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv3d.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7e4833cb-4dc2-4ef6-ade2-a88f244c9abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY FORMULAS FOR OUTPUT SIZE CALCULATION\n",
      "============================================================\n",
      "\n",
      "nn.Linear:\n",
      "  Output: (*, out_features)\n",
      "  The transformation is applied to the LAST dimension only\n",
      "\n",
      "nn.Conv1d/Conv2d/Conv3d:\n",
      "  For each spatial dimension:\n",
      "  L_out = floor((L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1)\n",
      "\n",
      "Special cases:\n",
      "  - padding='same' with stride=1: output size = input size\n",
      "  - padding=k//2 with odd kernel k and stride=1: preserves size\n",
      "  - stride=2 with appropriate padding: halves spatial dimensions\n"
     ]
    }
   ],
   "source": [
    "# Key formulas for output size calculation\n",
    "print(\"KEY FORMULAS FOR OUTPUT SIZE CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"nn.Linear:\")\n",
    "print(\"  Output: (*, out_features)\")\n",
    "print(\"  The transformation is applied to the LAST dimension only\")\n",
    "print()\n",
    "print(\"nn.Conv1d/Conv2d/Conv3d:\")\n",
    "print(\"  For each spatial dimension:\")\n",
    "print(\"  L_out = floor((L_in + 2*padding - dilation*(kernel_size-1) - 1) / stride + 1)\")\n",
    "print()\n",
    "print(\"Special cases:\")\n",
    "print(\"  - padding='same' with stride=1: output size = input size\")\n",
    "print(\"  - padding=k//2 with odd kernel k and stride=1: preserves size\")\n",
    "print(\"  - stride=2 with appropriate padding: halves spatial dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c9bf3-11ab-43ef-bea0-65e64cf05029",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transposed Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7cfbe0-4714-4ed4-9d17-a809b3e83038",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Transposed convolutional layers (often called \"deconvolutions\" or \"fractionally strided convolutions\") are essential building blocks for **upsampling** in neural networks. They perform the **inverse spatial transformation** of regular convolutions, where convolutions typically reduce spatial dimensions, transposed convolutions increase them.\n",
    "\n",
    "**Why Transposed Convolutions?**\n",
    "\n",
    "\n",
    "\n",
    "Regular convolutions with `stride > 1` **downsample** spatial dimensions. For many applications (image generation, segmentation, autoencoders), we need to **upsample**, to go from a smaller spatial representation back to a larger one.\n",
    "\n",
    "**Key insight**: A transposed convolution is not literally the inverse of a convolution. Instead, it is the **transpose of the convolution operation viewed as a matrix multiplication**. If a convolution maps from a space of dimension $N$ to a space of dimension $M$, the transposed convolution maps from $M$ back to $N$.\n",
    "\n",
    "Concretely:\n",
    "- **Convolution** ($\\text{stride}=2$): Input $L_{\\text{in}} \\to$ Output $L_{\\text{out}} \\approx L_{\\text{in}}/2$ (downsampling)\n",
    "- **Transposed Convolution** ($\\text{stride}=2$): Input $L_{\\text{in}} \\to$ Output $L_{\\text{out}} \\approx 2 \\cdot L_{\\text{in}}$ (upsampling)\n",
    "\n",
    "The `output_padding` parameter resolves the ambiguity that arises because multiple input sizes can map to the same output size under convolution.\n",
    "\n",
    "In what follows, we investigate the transposed convolutional layer types in PyTorch:\n",
    "\n",
    "1. **nn.ConvTranspose1d**: 1D transposed convolutions for sequential/temporal upsampling\n",
    "2. **nn.ConvTranspose2d**: 2D transposed convolutions for image upsampling\n",
    "3. **nn.ConvTranspose3d**: 3D transposed convolutions for volumetric/video upsampling\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa4cce7-d999-49c3-abf7-0e2812f722cd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before we start discussing the types of transposed convolutional layers, we briefly discuss the key parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `stride` | Controls the **upsampling factor**. Unlike in `Conv`, larger stride means **larger** output. |\n",
    "| `padding` | Reduces the output size (removes elements from the output). |\n",
    "| `output_padding` | Adds extra elements to one side of the output to resolve size ambiguity. Must be `< stride`. |\n",
    "| `dilation` | Spacing between kernel elements (same as in regular convolution). |\n",
    "\n",
    "**Output size formula** (for each spatial dimension):\n",
    "\n",
    "$$\n",
    "L_{\\text{out}} = (L_{\\text{in}} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1\n",
    "$$\n",
    "\n",
    "**Common recipe for exact $2\\times$ upsampling** (doubling spatial dimensions):\n",
    "\n",
    "```python\n",
    "# For exact 2x upsampling:\n",
    "nn.ConvTranspose2d(in_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "# or\n",
    "nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62599735-707b-48bb-9f35-5535e2dfc5c3",
   "metadata": {},
   "source": [
    "### nn.ConvTranspose1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2174ff-c0bf-45a8-9b0c-dbbaa4e45a0f",
   "metadata": {},
   "source": [
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose1d` layer applies a 1D transposed convolution over an input signal composed of several input planes. It is primarily used for:\n",
    "\n",
    "- **Audio generation**: WaveNet decoders, audio super-resolution\n",
    "- **Sequence upsampling**: Increasing temporal resolution of time series\n",
    "- **1D autoencoders**: Decoder networks for sequential data\n",
    "- **Signal reconstruction**: Upsampling compressed signal representations\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, L_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $L_{\\text{in}}$ = length of the input sequence\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, L_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $L_{\\text{out}} = (L_{\\text{in}} - 1) \\times \\text{stride} - 2 \\times \\text{padding} + \\text{dilation} \\times (\\text{kernel\\_size} - 1) + \\text{output\\_padding} + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int): Zero-padding added to both sides of the input. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int): Additional size added to one side of the output. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size})$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv1d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, L_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, L_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, i]$ to $y_{\\rm out}[b, c, j]$ can be understood as follows:\n",
    "\n",
    "The transposed convolution is defined as the **gradient of a convolution with respect to its input**. Equivalently, it can be viewed as:\n",
    "\n",
    "1. **Insert zeros** between input elements (determined by `stride`)\n",
    "2. **Pad** the expanded input\n",
    "3. Apply a **regular convolution** with the spatially flipped kernel\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, j] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k=0}^{K-1} \\tilde{x}_{\\rm in}[b, c_{\\rm in}, j + p - d \\cdot k] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K - 1 - k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s-1$ zeros between each element) and appropriate padding adjustment.\n",
    "\n",
    "Alternatively, the operation can be expressed as a **summation over input positions** that contribute to each output position:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, j] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{i=0}^{L_{\\rm in}-1} \\sum_{k=0}^{K-1} \\mathbf{1}_{[j = s \\cdot i + d \\cdot k - p + p_{\\rm out}]} \\cdot x_{\\rm in}[b, c_{\\rm in}, i] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $s = $ `stride`, $d = $ `dilation`, $p = $ `padding`, $p_{\\rm out} = $ `output_padding`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remark**: The key intuition is that each input element \"broadcasts\" its value to multiple output positions through the kernel, with the kernel flipped spatially.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "445472ad-02ea-45ff-ad80-9fe423b49c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose1d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3\n",
    "conv_transpose1d = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose1d.weight.shape}\")  # (in_channels, out_channels, kernel_size)\n",
    "print(f\"Bias shape: {conv_transpose1d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ca602d0b-9680-4422-9a79-8ca944f1e764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 52])\n",
      "Expected L_out: 52\n",
      "Note: Output is LARGER than input: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (upsampling)\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "out_channels = 33\n",
    "kernel_size = 3\n",
    "L_in = 50  # Input sequence length\n",
    "\n",
    "conv_transpose1d = nn.ConvTranspose1d(in_channels, out_channels, kernel_size)\n",
    "x = torch.randn(batch_size, in_channels, L_in)\n",
    "output = conv_transpose1d(x)\n",
    "\n",
    "# Calculate expected output length: L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\n",
    "# With defaults: stride=1, padding=0, dilation=1, output_padding=0\n",
    "L_out_expected = (L_in - 1) * 1 - 2*0 + 1*(kernel_size-1) + 0 + 1\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Expected L_out: {L_out_expected}\")\n",
    "print(f\"Note: Output is LARGER than input: {L_out_expected > x.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a3f6995-a969-4f91-838c-83b8fcf1520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 100])\n",
      "Upsampling factor: 2.0x\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling with stride=2\n",
    "conv_transpose1d_upsample = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_upsample(x)\n",
    "\n",
    "# L_out = (50-1)*2 - 2*1 + 1*(4-1) + 0 + 1 = 98 - 2 + 3 + 1 = 100\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Upsampling factor: {output.shape[2] / x.shape[2]}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4acab6e8-95f2-4424-aa9a-59953e8a3a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 100])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using output_padding to resolve ambiguity\n",
    "# When stride > 1, multiple input sizes can produce the same output size after Conv1d\n",
    "# output_padding helps specify the exact output size we want\n",
    "\n",
    "conv_transpose1d_op = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_op(x)\n",
    "\n",
    "# L_out = (50-1)*2 - 2*1 + 1*(3-1) + 1 + 1 = 98 - 2 + 2 + 1 + 1 = 100\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bbbe174f-0932-417b-b324-f1f9fc0a91c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 16, 100])\n",
      "After Conv1d (downsample): torch.Size([1, 32, 50])\n",
      "After ConvTranspose1d (upsample): torch.Size([1, 16, 100])\n",
      "Spatial dimension restored: True\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Comparison with Conv1d (inverse relationship)\n",
    "# ConvTranspose1d can reverse the spatial transformation of Conv1d\n",
    "\n",
    "L_original = 100\n",
    "x_original = torch.randn(1, 16, L_original)\n",
    "\n",
    "# Downsample with Conv1d\n",
    "conv1d_down = nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1)\n",
    "x_down = conv1d_down(x_original)\n",
    "\n",
    "# Upsample with ConvTranspose1d (matching parameters)\n",
    "conv_transpose1d_up = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "x_up = conv_transpose1d_up(x_down)\n",
    "\n",
    "print(f\"Original shape: {x_original.shape}\")\n",
    "print(f\"After Conv1d (downsample): {x_down.shape}\")\n",
    "print(f\"After ConvTranspose1d (upsample): {x_up.shape}\")\n",
    "print(f\"Spatial dimension restored: {x_original.shape[2] == x_up.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc282ca4-58f1-4071-8e7c-9e3773310b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50])\n",
      "Output shape: torch.Size([20, 33, 54])\n",
      "Effective kernel size with dilation=2: 5\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Dilated transposed convolution\n",
    "conv_transpose1d_dilated = nn.ConvTranspose1d(in_channels=16, out_channels=33, kernel_size=3, dilation=2)\n",
    "x = torch.randn(20, 16, 50)\n",
    "output = conv_transpose1d_dilated(x)\n",
    "\n",
    "# L_out = (50-1)*1 - 2*0 + 2*(3-1) + 0 + 1 = 49 + 4 + 1 = 54\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Effective kernel size with dilation=2: {2*(3-1)+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "06a9239b-da05-46db-acae-fc4924085005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: 52, Actual: 52\n",
      "  Match: True\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: 100, Actual: 100\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1}\n",
      "  Calculated: 100, Actual: 100\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2}\n",
      "  Calculated: 54, Actual: 54\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose1d\n",
    "def conv_transpose1d_output_size(L_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose1d output length\"\"\"\n",
    "    L_out = (L_in - 1) * stride - 2 * padding + dilation * (kernel_size - 1) + output_padding + 1\n",
    "    return L_out\n",
    "\n",
    "# Test with various configurations\n",
    "L_in = 50\n",
    "x = torch.randn(1, 16, L_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose1d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    L_out_calc = conv_transpose1d_output_size(L_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: {L_out_calc}, Actual: {output.shape[2]}\")\n",
    "    print(f\"  Match: {L_out_calc == output.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c04965-4736-4a9f-8c16-a6903b33bbbe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.ConvTranspose2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04fe3dc-071b-4d30-929c-d9c3160d46db",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose2d` layer applies a 2D transposed convolution over an input image composed of several input planes. It is the backbone of image upsampling and is used in:\n",
    "\n",
    "- **Generative Adversarial Networks (GANs)**: Generator networks that upsample from latent vectors to images\n",
    "- **Semantic segmentation**: Decoder networks in U-Net, FCN, etc.\n",
    "- **Image super-resolution**: Upscaling low-resolution images\n",
    "- **Autoencoders**: Decoder part for image reconstruction\n",
    "- **Style transfer**: Reconstructing styled images at original resolution\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1$\n",
    "  - $W_{\\text{out}} = (W_{\\text{in}} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input image (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int or tuple): Zero-padding added to both sides. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int or tuple): Additional size added to one side of output. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1])$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv2d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K_h, K_w)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, h, w]$ to $y_{\\rm out}[b, c, h', w']$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, h', w'] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\tilde{x}_{\\rm in}[b, c_{\\rm in}, h' + p_h - d_h k_h, w' + p_w - d_w k_w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K_h - 1 - k_h, K_w - 1 - k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s_h - 1$ and $s_w - 1$ zeros between elements along height and width respectively).\n",
    "\n",
    "Equivalently, the operation can be expressed as a **summation over input positions**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, h', w'] = \\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{h=0}^{H_{\\rm in}-1} \\sum_{w=0}^{W_{\\rm in}-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\mathbf{1}_{\\substack{[h' = s_h h + d_h k_h - p_h + p_{\\rm out,h}] \\\\ [w' = s_w w + d_w k_w - p_w + p_{\\rm out,w}]}} \\cdot x_{\\rm in}[b, c_{\\rm in}, h, w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $(s_h, s_w) = $ `stride`, $(d_h, d_w) = $ `dilation`, $(p_h, p_w) = $ `padding`, $(p_{\\rm out,h}, p_{\\rm out,w}) = $ `output_padding`, $(K_h, K_w) = $ `kernel_size`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remarks**:\n",
    "1. Each input pixel \"scatters\" its influence to a $K_h \\times K_w$ region in the output, weighted by the kernel.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes $(K_h, K_w) = (k, k)$.\n",
    "3. The `output_padding` only adds to one side (not both), which is why it must be strictly less than `stride`.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "796c0e79-4822-4d40-b31d-54f9302c38f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose2d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3\n",
    "conv_transpose2d = nn.ConvTranspose2d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose2d.weight.shape}\")  # (in_channels, out_channels, kH, kW)\n",
    "print(f\"Bias shape: {conv_transpose2d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "586182ab-f7f7-4290-b010-b7dfb88fb19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 50, 100])\n",
      "Output shape: torch.Size([20, 33, 52, 102])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (slight upsampling)\n",
    "batch_size = 20\n",
    "in_channels = 16\n",
    "H, W = 50, 100\n",
    "\n",
    "conv_transpose2d = nn.ConvTranspose2d(16, 33, kernel_size=3)\n",
    "x = torch.randn(batch_size, in_channels, H, W)\n",
    "output = conv_transpose2d(x)\n",
    "\n",
    "# H_out = (50-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 52\n",
    "# W_out = (100-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 102\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "da3ab54a-8c9a-49d9-b578-2f4753ddf92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 32])\n",
      "Output shape: torch.Size([20, 33, 64, 64])\n",
      "Exact 2x upsampling: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling (common in GANs and decoders)\n",
    "# kernel_size=4, stride=2, padding=1 gives exact 2x upsampling\n",
    "conv_transpose2d_2x = nn.ConvTranspose2d(16, 33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(20, 16, 32, 32)\n",
    "output = conv_transpose2d_2x(x)\n",
    "\n",
    "# H_out = (32-1)*2 - 2*1 + 1*(4-1) + 0 + 1 = 62 - 2 + 3 + 1 = 64\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Exact 2x upsampling: {output.shape[2] == 2 * x.shape[2] and output.shape[3] == 2 * x.shape[3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e81c327-260d-4c3b-9373-2670b2f2d6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 64])\n",
      "Output shape: torch.Size([20, 33, 63, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Non-square kernels and unequal stride with padding\n",
    "conv_transpose2d_nonsq = nn.ConvTranspose2d(16, 33, kernel_size=(3, 5), stride=(2, 1), padding=(1, 2))\n",
    "x = torch.randn(20, 16, 32, 64)\n",
    "output = conv_transpose2d_nonsq(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "abb08c26-a496-4a48-a939-942ca14fc5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([20, 16, 32, 32])\n",
      "Output shape: torch.Size([20, 33, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Using output_padding to resolve ambiguity\n",
    "# Two different input sizes (63 and 64) would produce the same output (32) with Conv2d(stride=2)\n",
    "# output_padding helps specify we want 64, not 63\n",
    "\n",
    "conv_transpose2d_op = nn.ConvTranspose2d(16, 33, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "x = torch.randn(20, 16, 32, 32)\n",
    "output = conv_transpose2d_op(x)\n",
    "\n",
    "# H_out = (32-1)*2 - 2*1 + 1*(3-1) + 1 + 1 = 62 - 2 + 2 + 1 + 1 = 64\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aa579660-8f09-477d-bc4e-3a36737c97b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([1, 512, 4, 4])\n",
      "Generated image shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Example 5: GAN-style generator upsampling chain\n",
    "# Starting from a small spatial size and progressively upsampling\n",
    "\n",
    "class SimpleGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each layer doubles spatial dimensions: 4 -> 8 -> 16 -> 32 -> 64\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 4 -> 8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8 -> 16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 16 -> 32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # 32 -> 64\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "generator = SimpleGenerator()\n",
    "latent = torch.randn(1, 512, 4, 4)  # Starting from 4x4 feature map\n",
    "generated_image = generator(latent)\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Generated image shape: {generated_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "82d6083c-4c71-4df9-ac03-4a321d5aa25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([1, 3, 64, 64])\n",
      "After Conv2d (downsample): torch.Size([1, 64, 32, 32])\n",
      "After ConvTranspose2d (upsample): torch.Size([1, 3, 64, 64])\n",
      "Spatial dimensions restored: True\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Comparison of Conv2d and ConvTranspose2d (inverse spatial transform)\n",
    "H_original, W_original = 64, 64\n",
    "x_original = torch.randn(1, 3, H_original, W_original)\n",
    "\n",
    "# Downsample with Conv2d\n",
    "conv2d_down = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "x_down = conv2d_down(x_original)\n",
    "\n",
    "# Upsample with ConvTranspose2d (matching parameters)\n",
    "conv_transpose2d_up = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "x_up = conv_transpose2d_up(x_down)\n",
    "\n",
    "print(f\"Original shape: {x_original.shape}\")\n",
    "print(f\"After Conv2d (downsample): {x_down.shape}\")\n",
    "print(f\"After ConvTranspose2d (upsample): {x_up.shape}\")\n",
    "print(f\"Spatial dimensions restored: {x_original.shape[2:] == x_up.shape[2:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e8b9df5-6a21-4cbf-b6ce-049865419f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: (34, 34), Actual: (34, 34)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (64, 64), Actual: (64, 64)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1}\n",
      "  Calculated: (64, 64), Actual: (64, 64)\n",
      "  Match: True\n",
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2}\n",
      "  Calculated: (36, 36), Actual: (36, 36)\n",
      "  Match: True\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose2d\n",
    "def conv_transpose2d_output_size(H_in, W_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose2d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation)\n",
    "    if isinstance(output_padding, int):\n",
    "        output_padding = (output_padding, output_padding)\n",
    "    \n",
    "    H_out = (H_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "    W_out = (W_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "    return H_out, W_out\n",
    "\n",
    "# Test with various configurations\n",
    "H_in, W_in = 32, 32\n",
    "x = torch.randn(1, 16, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 2, 'padding': 1, 'output_padding': 1},\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0, 'dilation': 2},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose2d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    H_out_calc, W_out_calc = conv_transpose2d_output_size(H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a78ac6-7a2a-4085-97c2-567647921afa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### nn.ConvTranspose3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148712d9-a9ea-4d39-97cf-2a21419a89d9",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "#### Where is it used?\n",
    "\n",
    "The `nn.ConvTranspose3d` layer applies a 3D transposed convolution over an input signal composed of several input planes. It is used for:\n",
    "\n",
    "- **Video generation**: Upsampling in video GANs, video prediction models\n",
    "- **Medical image reconstruction**: Upsampling in 3D U-Net for CT/MRI segmentation\n",
    "- **3D object generation**: Voxel-based generative models\n",
    "- **Volumetric super-resolution**: Increasing resolution of 3D data\n",
    "\n",
    "#### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ where:\n",
    "  - $B$ = batch size\n",
    "  - $C_{\\text{in}}$ = number of input channels\n",
    "  - $D_{\\text{in}}$ = depth of the input (e.g., number of frames, slices)\n",
    "  - $H_{\\text{in}}$ = height of the input\n",
    "  - $W_{\\text{in}}$ = width of the input\n",
    "\n",
    "- **Output**: $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ where:\n",
    "  - $C_{\\text{out}}$ = number of output channels\n",
    "  - $D_{\\text{out}} = (D_{\\text{in}} - 1) \\times \\text{stride}[0] - 2 \\times \\text{padding}[0] + \\text{dilation}[0] \\times (\\text{kernel\\_size}[0] - 1) + \\text{output\\_padding}[0] + 1$\n",
    "  - $H_{\\text{out}} = (H_{\\text{in}} - 1) \\times \\text{stride}[1] - 2 \\times \\text{padding}[1] + \\text{dilation}[1] \\times (\\text{kernel\\_size}[1] - 1) + \\text{output\\_padding}[1] + 1$\n",
    "  - $W_{\\text{out}} = (W_{\\text{in}} - 1) \\times \\text{stride}[2] - 2 \\times \\text{padding}[2] + \\text{dilation}[2] \\times (\\text{kernel\\_size}[2] - 1) + \\text{output\\_padding}[2] + 1$\n",
    "\n",
    "#### Default Arguments\n",
    "```python\n",
    "nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
    "                   output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros')\n",
    "```\n",
    "\n",
    "- `in_channels` (int): Number of channels in the input volume (required)\n",
    "- `out_channels` (int): Number of channels produced by the convolution (required)\n",
    "- `kernel_size` (int or tuple): Size of the convolving kernel (required)\n",
    "- `stride` (int or tuple): Stride of the convolution (controls upsampling factor). Default: `1`\n",
    "- `padding` (int or tuple): Zero-padding added to all sides. **Reduces** output size. Default: `0`\n",
    "- `output_padding` (int or tuple): Additional size added to one side. Must be `< stride`. Default: `0`\n",
    "- `dilation` (int or tuple): Spacing between kernel elements. Default: `1`\n",
    "- `groups` (int): Number of blocked connections. Default: `1`\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- `padding_mode` (str): Only `'zeros'` is supported. Default: `'zeros'`\n",
    "\n",
    "#### Weight Shape\n",
    "\n",
    "The weight tensor has shape: $(C_{\\text{in}}, \\frac{C_{\\text{out}}}{\\text{groups}}, \\text{kernel\\_size}[0], \\text{kernel\\_size}[1], \\text{kernel\\_size}[2])$\n",
    "\n",
    "**Note**: The weight shape is **transposed** compared to `nn.Conv3d` which has shape $(C_{\\text{out}}, \\frac{C_{\\text{in}}}{\\text{groups}}, K_d, K_h, K_w)$.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "For an input of size $(B, C_{\\text{in}}, D_{\\text{in}}, H_{\\text{in}}, W_{\\text{in}})$ and output of size $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$, the transposed convolution from $x_{\\rm in}[b, c, d, h, w]$ to $y_{\\rm out}[b, c, d', h', w']$ is performed as follows:\n",
    "\n",
    "* For `groups`$ = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d', h', w'] = &\\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{k_d=0}^{K_d-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\\\\n",
    "&\\tilde{x}_{\\rm in}[b, c_{\\rm in}, d' + p_d - d_d k_d, h' + p_h - d_h k_h, w' + p_w - d_w k_w] \\\\\n",
    "&\\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K_d - 1 - k_d, K_h - 1 - k_h, K_w - 1 - k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\tilde{x}_{\\rm in}$ is the input after zero-insertion (inserting $s_d - 1$, $s_h - 1$, and $s_w - 1$ zeros between elements along depth, height, and width respectively).\n",
    "\n",
    "Equivalently, as a **summation over input positions**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y_{\\rm out}[b, c_{\\rm out}, d', h', w'] = &\\sum_{c_{\\rm in}=0}^{C_{\\rm in}-1} \\sum_{d=0}^{D_{\\rm in}-1} \\sum_{h=0}^{H_{\\rm in}-1} \\sum_{w=0}^{W_{\\rm in}-1} \\sum_{k_d=0}^{K_d-1} \\sum_{k_h=0}^{K_h-1} \\sum_{k_w=0}^{K_w-1} \\\\\n",
    "&\\mathbf{1}_{\\substack{[d' = s_d d + d_d k_d - p_d + p_{\\rm out,d}] \\\\ [h' = s_h h + d_h k_h - p_h + p_{\\rm out,h}] \\\\ [w' = s_w w + d_w k_w - p_w + p_{\\rm out,w}]}} \\cdot x_{\\rm in}[b, c_{\\rm in}, d, h, w] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k_d, k_h, k_w] + \\beta[c_{\\rm out}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $(s_d, s_h, s_w) = $ `stride`, $(d_d, d_h, d_w) = $ `dilation`, $(p_d, p_h, p_w) = $ `padding`, $(p_{\\rm out,d}, p_{\\rm out,h}, p_{\\rm out,w}) = $ `output_padding`, $(K_d, K_h, K_w) = $ `kernel_size`, and $\\mathbf{1}_{[\\cdot]}$ is the indicator function.\n",
    "\n",
    "* For `groups`$ > 1$, the channels are split into groups analogously to regular convolutions.\n",
    "\n",
    "**Remarks**:\n",
    "1. Each input voxel \"scatters\" its influence to a $K_d \\times K_h \\times K_w$ region in the output.\n",
    "2. If `kernel_size = k` is just a number, then PyTorch automatically assumes $(K_d, K_h, K_w) = (k, k, k)$.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "255f93eb-25d3-4510-bcd0-3410c21e447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([16, 33, 3, 3, 3])\n",
      "Bias shape: torch.Size([33])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.ConvTranspose3d usage\n",
    "# Input: 16 channels, Output: 33 channels, kernel size: 3x3x3\n",
    "conv_transpose3d = nn.ConvTranspose3d(in_channels=16, out_channels=33, kernel_size=3)\n",
    "\n",
    "print(f\"Weight shape: {conv_transpose3d.weight.shape}\")  # (in_channels, out_channels, kD, kH, kW)\n",
    "print(f\"Bias shape: {conv_transpose3d.bias.shape}\")      # (out_channels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ed1f29a-b6c0-44ad-b461-cf4a904805fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 10, 50, 100])\n",
      "Output shape: torch.Size([4, 33, 12, 52, 102])\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Basic transposed convolution (slight upsampling)\n",
    "conv_transpose3d = nn.ConvTranspose3d(16, 33, kernel_size=3)\n",
    "x = torch.randn(4, 16, 10, 50, 100)  # (batch, channels, depth, height, width)\n",
    "output = conv_transpose3d(x)\n",
    "\n",
    "# D_out = (10-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 12\n",
    "# H_out = (50-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 52\n",
    "# W_out = (100-1)*1 - 2*0 + 1*(3-1) + 0 + 1 = 102\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c156b2bf-d5a8-4229-962f-f72d96b8233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 8, 16, 16])\n",
      "Output shape: torch.Size([4, 33, 16, 32, 32])\n",
      "Exact 2x upsampling in all dims: True\n"
     ]
    }
   ],
   "source": [
    "# Example 2: 2x upsampling in all dimensions\n",
    "conv_transpose3d_2x = nn.ConvTranspose3d(16, 33, kernel_size=4, stride=2, padding=1)\n",
    "x = torch.randn(4, 16, 8, 16, 16)\n",
    "output = conv_transpose3d_2x(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Exact 2x upsampling in all dims: {all(o == 2*i for o, i in zip(output.shape[2:], x.shape[2:]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8b6a18dd-1217-4d89-850f-25b224eabd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 16, 16, 32, 32])\n",
      "Output shape: torch.Size([4, 33, 16, 64, 64])\n",
      "Temporal dim preserved: True\n",
      "Spatial dims doubled: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Non-cubic kernels and unequal stride (e.g., upsample only spatially, not temporally)\n",
    "# kernel_size=(depth, height, width), stride=(sD, sH, sW), padding=(pD, pH, pW)\n",
    "conv_transpose3d_spatial = nn.ConvTranspose3d(16, 33, kernel_size=(3, 4, 4), stride=(1, 2, 2), padding=(1, 1, 1))\n",
    "x = torch.randn(4, 16, 16, 32, 32)  # Video: 16 frames, 32x32 resolution\n",
    "output = conv_transpose3d_spatial(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Temporal dim preserved: {x.shape[2] == output.shape[2]}\")\n",
    "print(f\"Spatial dims doubled: {output.shape[3] == 2*x.shape[3] and output.shape[4] == 2*x.shape[4]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "64cbc4dd-fd4b-4dd9-ae1a-1f9d76c4f446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bottleneck features: torch.Size([1, 128, 8, 8, 8])\n",
      "Skip connection: torch.Size([1, 64, 16, 16, 16])\n",
      "Decoder output: torch.Size([1, 64, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Medical imaging - 3D U-Net decoder block\n",
    "# Upsampling feature maps in a 3D segmentation network\n",
    "\n",
    "class UNet3DDecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.ConvTranspose3d(in_channels, out_channels, \n",
    "                                            kernel_size=2, stride=2)  # 2x upsample\n",
    "        self.conv = nn.Conv3d(out_channels * 2, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm3d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, skip_connection):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip_connection], dim=1)  # Skip connection from encoder\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "# Simulate decoder block usage\n",
    "decoder_block = UNet3DDecoderBlock(128, 64)\n",
    "bottleneck_features = torch.randn(1, 128, 8, 8, 8)  # From encoder bottleneck\n",
    "skip_features = torch.randn(1, 64, 16, 16, 16)      # Skip connection from encoder\n",
    "output = decoder_block(bottleneck_features, skip_features)\n",
    "\n",
    "print(f\"Bottleneck features: {bottleneck_features.shape}\")\n",
    "print(f\"Skip connection: {skip_features.shape}\")\n",
    "print(f\"Decoder output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0b21d8dd-2f35-40b5-b7c6-68e6c14ffea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([1, 256, 2, 4, 4])\n",
      "Generated video shape: torch.Size([1, 3, 16, 32, 32])\n",
      "(batch, RGB channels, frames, height, width)\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Video generation - upsampling from latent to video frames\n",
    "# Starting from a compressed spatiotemporal representation\n",
    "\n",
    "class VideoGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Progressively upsample: (2,4,4) -> (4,8,8) -> (8,16,16) -> (16,32,32)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose3d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose3d(64, 3, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "generator = VideoGenerator()\n",
    "latent = torch.randn(1, 256, 2, 4, 4)  # Compressed representation\n",
    "video = generator(latent)\n",
    "\n",
    "print(f\"Latent shape: {latent.shape}\")\n",
    "print(f\"Generated video shape: {video.shape}\")\n",
    "print(f\"(batch, RGB channels, frames, height, width)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d817fd49-ae70-4cd1-938e-4edd78dcc7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'kernel_size': 3, 'stride': 1, 'padding': 0}\n",
      "  Calculated: (10, 18, 18), Actual: (10, 18, 18)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 4, 'stride': 2, 'padding': 1}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': (3, 4, 4), 'stride': (1, 2, 2), 'padding': (1, 1, 1)}\n",
      "  Calculated: (8, 32, 32), Actual: (8, 32, 32)\n",
      "  Match: True\n",
      "\n",
      "Config: {'kernel_size': 2, 'stride': 2, 'padding': 0}\n",
      "  Calculated: (16, 32, 32), Actual: (16, 32, 32)\n",
      "  Match: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify the output size formula for ConvTranspose3d\n",
    "def conv_transpose3d_output_size(D_in, H_in, W_in, kernel_size, stride=1, padding=0, dilation=1, output_padding=0):\n",
    "    \"\"\"Calculate ConvTranspose3d output dimensions\"\"\"\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = (kernel_size, kernel_size, kernel_size)\n",
    "    if isinstance(stride, int):\n",
    "        stride = (stride, stride, stride)\n",
    "    if isinstance(padding, int):\n",
    "        padding = (padding, padding, padding)\n",
    "    if isinstance(dilation, int):\n",
    "        dilation = (dilation, dilation, dilation)\n",
    "    if isinstance(output_padding, int):\n",
    "        output_padding = (output_padding, output_padding, output_padding)\n",
    "    \n",
    "    D_out = (D_in - 1) * stride[0] - 2 * padding[0] + dilation[0] * (kernel_size[0] - 1) + output_padding[0] + 1\n",
    "    H_out = (H_in - 1) * stride[1] - 2 * padding[1] + dilation[1] * (kernel_size[1] - 1) + output_padding[1] + 1\n",
    "    W_out = (W_in - 1) * stride[2] - 2 * padding[2] + dilation[2] * (kernel_size[2] - 1) + output_padding[2] + 1\n",
    "    return D_out, H_out, W_out\n",
    "\n",
    "# Test\n",
    "D_in, H_in, W_in = 8, 16, 16\n",
    "x = torch.randn(1, 16, D_in, H_in, W_in)\n",
    "\n",
    "configs = [\n",
    "    {'kernel_size': 3, 'stride': 1, 'padding': 0},\n",
    "    {'kernel_size': 4, 'stride': 2, 'padding': 1},\n",
    "    {'kernel_size': (3, 4, 4), 'stride': (1, 2, 2), 'padding': (1, 1, 1)},\n",
    "    {'kernel_size': 2, 'stride': 2, 'padding': 0},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    conv_t = nn.ConvTranspose3d(16, 33, **config)\n",
    "    output = conv_t(x)\n",
    "    D_out_calc, H_out_calc, W_out_calc = conv_transpose3d_output_size(D_in, H_in, W_in, **config)\n",
    "    print(f\"Config: {config}\")\n",
    "    print(f\"  Calculated: ({D_out_calc}, {H_out_calc}, {W_out_calc}), Actual: {tuple(output.shape[2:])}\")\n",
    "    print(f\"  Match: {(D_out_calc, H_out_calc, W_out_calc) == tuple(output.shape[2:])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec2f32-0350-4d35-a0df-692ab081d158",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6250dad-c2af-4ace-8d5f-cf3770cf7485",
   "metadata": {},
   "source": [
    "#### Note on the Mathematical Formulation of Transposed Convolutions\n",
    "\n",
    "Throughout this notebook, we presented two equivalent formulations for transposed convolutions:\n",
    "\n",
    "**Formulation 1 (Flipped Kernel View):**\n",
    "$$\n",
    "y_{\\rm out}[\\ldots, j] = \\sum_{c_{\\rm in}, k} \\tilde{x}_{\\rm in}[\\ldots, j + p - d \\cdot k] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, K - 1 - k] + \\beta[c_{\\rm out}]\n",
    "$$\n",
    "\n",
    "**Formulation 2 (Scatter/Adjoint View):**\n",
    "$$\n",
    "y_{\\rm out}[\\ldots, j] = \\sum_{c_{\\rm in}, i, k} \\mathbf{1}_{[j = s \\cdot i + d \\cdot k - p + p_{\\rm out}]} \\cdot x_{\\rm in}[\\ldots, i] \\cdot \\omega[c_{\\rm in}, c_{\\rm out}, k] + \\beta[c_{\\rm out}]\n",
    "$$\n",
    "\n",
    "These formulations are **mathematically equivalent** via the change of variables $k' = K - 1 - k$. However, they represent different conceptual interpretations:\n",
    "\n",
    "- **Flipped Kernel View**: The transposed convolution can be understood as inserting zeros between input elements, then applying a regular convolution with a spatially flipped kernel. This interpretation arises naturally when deriving the transposed convolution as the gradient of a forward convolution.\n",
    "\n",
    "- **Scatter/Adjoint View**: Each input element \"scatters\" or \"broadcasts\" its value to multiple output positions, weighted by the kernel elements. This is the **interpretation that matches PyTorch's actual implementation** — the kernel weights are used directly without spatial flipping.\n",
    "\n",
    "Lets verify this behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9e544fc6-f52d-47d9-a5be-d5d20969adc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create a ConvTranspose1d with a known kernel\n",
    "conv_t = nn.ConvTranspose1d(1, 1, kernel_size=3, bias=False)\n",
    "conv_t.weight.data = torch.tensor([[[1., 2., 3.]]])  # kernel = [1, 2, 3]\n",
    "\n",
    "# Single input\n",
    "x = torch.tensor([[[1.]]])  # single value at position 0\n",
    "output = conv_t(x)\n",
    "print(output)  # tensor([[[1., 2., 3.]]])  — NOT [3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "aacb162f-f8cd-47e3-89e1-25aa12968cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSPOSED CONVOLUTIONAL LAYER COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (32, 64, 50) -> (batch, in_channels, length)\n",
      "  Output shape: (32, 128, 100) -> (batch, out_channels, length_upsampled)\n",
      "  Weight shape: (64, 128, 4) -> (in_ch, out_ch, kernel)\n",
      "  Parameters:   32,896\n",
      "\n",
      "nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (32, 64, 28, 28) -> (batch, in_channels, H, W)\n",
      "  Output shape: (32, 128, 56, 56) -> (batch, out_channels, H_up, W_up)\n",
      "  Weight shape: (64, 128, 4, 4) -> (in_ch, out_ch, kH, kW)\n",
      "  Parameters:   131,200\n",
      "\n",
      "nn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1):\n",
      "  Input shape:  (4, 64, 8, 14, 14) -> (batch, in_channels, D, H, W)\n",
      "  Output shape: (4, 128, 16, 28, 28) -> (batch, out_channels, D_up, H_up, W_up)\n",
      "  Weight shape: (64, 128, 4, 4, 4) -> (in_ch, out_ch, kD, kH, kW)\n",
      "  Parameters:   524,416\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary comparison of all transposed convolutional layers\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSPOSED CONVOLUTIONAL LAYER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# nn.ConvTranspose1d\n",
    "conv_t1d = nn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_1d = torch.randn(32, 64, 50)\n",
    "print(f\"\\nnn.ConvTranspose1d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_1d.shape)} -> (batch, in_channels, length)\")\n",
    "print(f\"  Output shape: {tuple(conv_t1d(x_1d).shape)} -> (batch, out_channels, length_upsampled)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t1d.weight.shape)} -> (in_ch, out_ch, kernel)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t1d.parameters()):,}\")\n",
    "\n",
    "# nn.ConvTranspose2d\n",
    "conv_t2d = nn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_2d = torch.randn(32, 64, 28, 28)\n",
    "print(f\"\\nnn.ConvTranspose2d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_2d.shape)} -> (batch, in_channels, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv_t2d(x_2d).shape)} -> (batch, out_channels, H_up, W_up)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t2d.weight.shape)} -> (in_ch, out_ch, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t2d.parameters()):,}\")\n",
    "\n",
    "# nn.ConvTranspose3d\n",
    "conv_t3d = nn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1)\n",
    "x_3d = torch.randn(4, 64, 8, 14, 14)\n",
    "print(f\"\\nnn.ConvTranspose3d(64, 128, kernel_size=4, stride=2, padding=1):\")\n",
    "print(f\"  Input shape:  {tuple(x_3d.shape)} -> (batch, in_channels, D, H, W)\")\n",
    "print(f\"  Output shape: {tuple(conv_t3d(x_3d).shape)} -> (batch, out_channels, D_up, H_up, W_up)\")\n",
    "print(f\"  Weight shape: {tuple(conv_t3d.weight.shape)} -> (in_ch, out_ch, kD, kH, kW)\")\n",
    "print(f\"  Parameters:   {sum(p.numel() for p in conv_t3d.parameters()):,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b753f269-1fc9-4e32-8d94-68548751e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEY FORMULAS FOR OUTPUT SIZE CALCULATION\n",
      "============================================================\n",
      "\n",
      "nn.ConvTranspose1d/ConvTranspose2d/ConvTranspose3d:\n",
      "  For each spatial dimension:\n",
      "  L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\n",
      "\n",
      "Common recipes for exact 2x upsampling:\n",
      "  - kernel_size=4, stride=2, padding=1 -> exact doubling\n",
      "  - kernel_size=2, stride=2, padding=0 -> exact doubling\n",
      "  - kernel_size=3, stride=2, padding=1, output_padding=1 -> exact doubling\n",
      "\n",
      "Key differences from regular Conv:\n",
      "  - stride > 1 INCREASES output size (upsampling)\n",
      "  - padding DECREASES output size\n",
      "  - Weight shape is (in_ch, out_ch, ...) not (out_ch, in_ch, ...)\n"
     ]
    }
   ],
   "source": [
    "# Key formulas for output size calculation\n",
    "print(\"KEY FORMULAS FOR OUTPUT SIZE CALCULATION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"nn.ConvTranspose1d/ConvTranspose2d/ConvTranspose3d:\")\n",
    "print(\"  For each spatial dimension:\")\n",
    "print(\"  L_out = (L_in - 1) * stride - 2*padding + dilation*(kernel_size-1) + output_padding + 1\")\n",
    "print()\n",
    "print(\"Common recipes for exact 2x upsampling:\")\n",
    "print(\"  - kernel_size=4, stride=2, padding=1 -> exact doubling\")\n",
    "print(\"  - kernel_size=2, stride=2, padding=0 -> exact doubling\")\n",
    "print(\"  - kernel_size=3, stride=2, padding=1, output_padding=1 -> exact doubling\")\n",
    "print()\n",
    "print(\"Key differences from regular Conv:\")\n",
    "print(\"  - stride > 1 INCREASES output size (upsampling)\")\n",
    "print(\"  - padding DECREASES output size\")\n",
    "print(\"  - Weight shape is (in_ch, out_ch, ...) not (out_ch, in_ch, ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7cb560b-6c48-4d4f-bb36-2b981220d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONV vs CONVTRANSPOSE: INVERSE SPATIAL TRANSFORMATIONS\n",
      "============================================================\n",
      "\n",
      "1D Example:\n",
      "  Original: (1, 16, 100)\n",
      "  After Conv1d(stride=2): (1, 32, 50)\n",
      "  After ConvTranspose1d(stride=2): (1, 16, 100)\n",
      "\n",
      "2D Example:\n",
      "  Original: (1, 3, 64, 64)\n",
      "  After Conv2d(stride=2): (1, 64, 32, 32)\n",
      "  After ConvTranspose2d(stride=2): (1, 3, 64, 64)\n",
      "\n",
      "3D Example:\n",
      "  Original: (1, 3, 16, 32, 32)\n",
      "  After Conv3d(stride=2): (1, 64, 8, 16, 16)\n",
      "  After ConvTranspose3d(stride=2): (1, 3, 16, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Comparison: Conv vs ConvTranspose (inverse spatial transformations)\n",
    "print(\"CONV vs CONVTRANSPOSE: INVERSE SPATIAL TRANSFORMATIONS\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# 1D example\n",
    "print(\"1D Example:\")\n",
    "x1d = torch.randn(1, 16, 100)\n",
    "conv1d = nn.Conv1d(16, 32, kernel_size=4, stride=2, padding=1)\n",
    "conv_t1d = nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x1d.shape)}\")\n",
    "print(f\"  After Conv1d(stride=2): {tuple(conv1d(x1d).shape)}\")\n",
    "print(f\"  After ConvTranspose1d(stride=2): {tuple(conv_t1d(conv1d(x1d)).shape)}\")\n",
    "print()\n",
    "\n",
    "# 2D example\n",
    "print(\"2D Example:\")\n",
    "x2d = torch.randn(1, 3, 64, 64)\n",
    "conv2d = nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv_t2d = nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x2d.shape)}\")\n",
    "print(f\"  After Conv2d(stride=2): {tuple(conv2d(x2d).shape)}\")\n",
    "print(f\"  After ConvTranspose2d(stride=2): {tuple(conv_t2d(conv2d(x2d)).shape)}\")\n",
    "print()\n",
    "\n",
    "# 3D example\n",
    "print(\"3D Example:\")\n",
    "x3d = torch.randn(1, 3, 16, 32, 32)\n",
    "conv3d = nn.Conv3d(3, 64, kernel_size=4, stride=2, padding=1)\n",
    "conv_t3d = nn.ConvTranspose3d(64, 3, kernel_size=4, stride=2, padding=1)\n",
    "print(f\"  Original: {tuple(x3d.shape)}\")\n",
    "print(f\"  After Conv3d(stride=2): {tuple(conv3d(x3d).shape)}\")\n",
    "print(f\"  After ConvTranspose3d(stride=2): {tuple(conv_t3d(conv3d(x3d)).shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d4502716-061f-46dd-ac99-2160c3574c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 3.]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8fd4a7-1773-452a-a6fd-b9155070124e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
