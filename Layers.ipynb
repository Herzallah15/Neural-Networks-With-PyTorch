{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8a6232-fbdd-466a-a420-4fc50b79008a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x115725270>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31a0ead-bc14-4d64-99a4-d89ae7c7a1ef",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">Neural network layers are the fundamental building blocks that transform input data into meaningful representations. Each layer type is designed for specific data structures and tasks. The **shape of the input and output tensors** plays a crucial role in understanding how these layers operate.\n",
    "\n",
    "In what follows, we investigate the most common layer types in PyTorch:\n",
    "\n",
    "1. **nn.Linear**: Fully connected layers for general-purpose transformations\n",
    "2. **nn.Conv1d**: 1D convolutions for sequential/temporal data\n",
    "3. **nn.Conv2d**: 2D convolutions for image data\n",
    "4. **nn.Conv3d**: 3D convolutions for volumetric/video data\n",
    "\n",
    "Throughout this notebook:\n",
    "- $B$ denotes the batch size\n",
    "- $C_{\\text{in}}$ and $C_{\\text{out}}$ denote input and output channels/features\n",
    "- Spatial dimensions are denoted by $L$ (length), $H$ (height), $W$ (width), $D$ (depth)\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19674a-3b68-45ec-923f-911e7192c272",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "| Layer | Input Shape | Output Shape | Primary Use Cases | Key Parameters |\n",
    "|-------|-------------|--------------|-------------------|----------------|\n",
    "| **nn.Linear** | $(*, H_{\\text{in}})$ | $(*, H_{\\text{out}})$ | MLPs, classification heads, dense connections, Transformer projections | `in_features`, `out_features`, `bias` |\n",
    "| **nn.Conv1d** | $(B, C_{\\text{in}}, L)$ | $(B, C_{\\text{out}}, L_{\\text{out}})$ | Time series, audio, text (1D sequences) | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding` |\n",
    "| **nn.Conv2d** | $(B, C_{\\text{in}}, H, W)$ | $(B, C_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Images, feature maps, 2D spatial data | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "| **nn.Conv3d** | $(B, C_{\\text{in}}, D, H, W)$ | $(B, C_{\\text{out}}, D_{\\text{out}}, H_{\\text{out}}, W_{\\text{out}})$ | Videos, medical imaging (CT/MRI), 3D point clouds | `in_channels`, `out_channels`, `kernel_size`, `stride`, `padding`, `dilation`, `groups` |\n",
    "\n",
    "Detailed explanations of each layer, including mathematical formulations and implementation examples, follow below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913c152-b2d7-401b-b573-3dc034a3956d",
   "metadata": {},
   "source": [
    "## nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184a791-9132-4eb1-9ea3-1b2dbb235a57",
   "metadata": {},
   "source": [
    "<span style=\"font-size: 15px;\">\n",
    "\n",
    "### Where is it used?\n",
    "\n",
    "The `nn.Linear` layer (also known as a fully connected or dense layer) applies a linear transformation to the incoming data. It is the most fundamental building block in neural networks and is used in:\n",
    "\n",
    "- **Multi-Layer Perceptrons (MLPs)**: The backbone of simple feedforward networks\n",
    "- **Classification heads**: Final layers that map features to class logits\n",
    "- **Transformer architectures**: Q, K, V projections and feed-forward networks\n",
    "- **Autoencoders**: Encoding and decoding dense representations\n",
    "- **Regression tasks**: Mapping features to continuous outputs\n",
    "\n",
    "### Input and Output Shapes\n",
    "\n",
    "- **Input**: $(*, H_{\\text{in}})$ where $*$ means any number of dimensions and $H_{\\text{in}}$ is the number of input features\n",
    "- **Output**: $(*, H_{\\text{out}})$ where all dimensions except the last remain unchanged\n",
    "\n",
    "**Important**: The linear transformation is applied to the **last dimension** only.\n",
    "\n",
    "### Default Arguments\n",
    "\n",
    "```python\n",
    "nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)\n",
    "```\n",
    "\n",
    "- `in_features` (int): Size of each input sample (required)\n",
    "- `out_features` (int): Size of each output sample (required)\n",
    "- `bias` (bool): If `True`, adds a learnable bias. Default: `True`\n",
    "- \n",
    "### Mathematical Formulation\n",
    "\n",
    "The linear layer applies the following transformation:\n",
    "\n",
    "$$\n",
    "{y_{\\rm out}}^{i,j,\\cdots, k} = \\omega_{k z} \\, {x_{\\rm in}}^{i,j,\\cdots, z} + b^{k}\n",
    "$$\n",
    "\n",
    "\n",
    "where the indices \"$i,j,\\cdots$\" represent an arbitrary dimension, i.e., $*$. This makes the layer **batch-agnostic**. As we see,\n",
    "- $x$ is the input tensor of shape $(*, H_{\\text{in}})$\n",
    "- $\\omega$ is the weight matrix of shape $(H_{\\text{out}}, H_{\\text{in}})$\n",
    "- $b$ is the bias vector of shape $(H_{\\text{out}})$\n",
    "- $y$ is the output tensor of shape $(*, H_{\\text{out}})$\n",
    "- **Notice**, that, all elements in the arbirary dimension \"$i,j,\\cdots$\" get exactly the same biases and weights.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Weight Initialization\n",
    "\n",
    "Weights are initialized from $\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})$ where $k = \\frac{1}{H_{\\text{in}}}$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62e6aaa-46a3-4f58-b540-ccab2157f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape: torch.Size([30, 20])\n",
      "Bias shape: torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "# Basic nn.Linear usage\n",
    "# Create a linear layer that maps 20 input features to 30 output features\n",
    "linear = nn.Linear(20, 30)\n",
    "\n",
    "# Check the weight and bias shapes\n",
    "print(f\"Weight shape: {linear.weight.shape}\")  # (out_features, in_features)\n",
    "print(f\"Bias shape: {linear.bias.shape}\")      # (out_features,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67d299c4-0284-4d7a-b5e5-01cb687c227b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([128, 20])\n",
      "Output shape: torch.Size([128, 30])\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple 2D input (batch_size, in_features)\n",
    "batch_size = 128\n",
    "in_features = 20\n",
    "out_features = 30\n",
    "\n",
    "linear = nn.Linear(in_features, out_features)\n",
    "models_weights = linear.weight\n",
    "models_biases = linear.bias\n",
    "x = torch.randn(batch_size, in_features)\n",
    "output = linear(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Lets now try to reproduce the output manually:\n",
    "\n",
    "output_manual = torch.einsum('bi,oi->bo', x, models_weights) + models_biases.unsqueeze(0).expand(batch_size, -1)\n",
    "print(f'Outputs match: {torch.all(output_manual==output).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b58cd32-e699-4cd3-be86-d44e5e910935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has bias: False\n",
      "Outputs match: True\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Without bias\n",
    "linear_no_bias = nn.Linear(20, 30, bias=False)\n",
    "print(f\"Has bias: {linear_no_bias.bias is not None}\")\n",
    "\n",
    "# Verify the transformation manually\n",
    "x = torch.randn(5, 20)\n",
    "output_layer = linear_no_bias(x)\n",
    "output_manual = x @ linear_no_bias.weight.T  # y = x @ W^T\n",
    "\n",
    "print(f\"Outputs match: {torch.allclose(output_layer, output_manual)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
